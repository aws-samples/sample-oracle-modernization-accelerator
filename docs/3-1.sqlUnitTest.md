---
layout: default
title: 3-1. SQL Unit Test
nav_order: 1
parent: SQL Unit Test
description: "Oracle Migration Accelerator 테스트 프로그램들의 기능과 사용법"
---

# Oracle Migration Accelerator 프로그램 교육 자료

## 문서 정보

- **작성일:** 2025년 7월 29일
- **대상:** Oracle to PostgreSQL 마이그레이션 프로젝트 담당자
- **목적:** OMA 테스트 프로그램들의 기능과 사용법 상세 교육
- **버전:** 2.0 (상세 버전)

## 목차

1. [개요](#overview)
2. [프로그램 실행 흐름](#program-flow)
3. [환경 설정](#environment)
4. [initTest.sh - 메인 실행 스크립트](#init-script)
5. [개별 프로그램 상세](#programs)
   - [5.1 XMLToSQL.py](#xmltosql)
   - [5.2 GetDictionary.py](#getdictionary)
   - [5.3 BindSampler.py](#bindsampler)
   - [5.4 BindMapper.py](#bindmapper)
   - [5.5 SaveSQLToDB.py](#savesqltodb)
   - [5.6 ExecuteAndCompareSQL.py](#executecompare)
   - [5.7 analyze_pg_errors.py](#analyze-errors)
   - [5.8 pg_transform.py](#pg-transform)
6. [문제 해결](#troubleshooting)
7. [모범 사례](#best-practices)
8. [부록](#appendix)

---

## 1. 개요 {#overview}

Oracle Migration Accelerator (OMA) 테스트 프로그램들은 Oracle 데이터베이스에서 PostgreSQL로의 마이그레이션 과정에서 SQL 호환성을 테스트하고 검증하는 포괄적인 도구 모음입니다.

### 전체 아키텍처

- **입력:** MyBatis XML 매퍼 파일들
- **처리:** SQL 추출 → 바인드 변수 처리 → 실행 및 비교 → 에러 분석 → 자동 수정
- **출력:** 호환성 보고서, 수정된 SQL, 에러 분석 결과

### 핵심 가치

- **자동화:** 수동 작업을 최소화하여 효율성 극대화
- **정확성:** 체계적인 검증을 통한 높은 신뢰성
- **확장성:** 다양한 데이터베이스 타입 지원
- **추적성:** 모든 과정의 상세한 로깅

## 2. 프로그램 실행 흐름 {#program-flow}

### 단계별 실행 흐름

| 단계 | 프로그램 | 입력 | 출력 | 소요시간 (예상) |
|------|----------|------|------|----------------|
| 1 | initTest.sh | 환경 변수 | 초기화 완료 | 1-2분 |
| 2 | XMLToSQL.py | XML 매퍼 파일들 | 개별 SQL 파일들 | 5-10분 |
| 3 | GetDictionary.py | Oracle DB 연결 | all_dictionary.json | 10-30분 |
| 4 | BindSampler.py | SQL 파일들, 딕셔너리 | 바인드 변수 샘플 | 5-15분 |
| 5 | BindMapper.py | SQL + 샘플 | 실행 가능한 SQL | 5-10분 |
| 6 | SaveSQLToDB.py | 매핑된 SQL | sqllist 테이블 | 2-5분 |
| 7 | ExecuteAndCompareSQL.py | sqllist 테이블 | 실행 결과 | 30분-2시간 |
| 8 | analyze_pg_errors.py | 실행 결과 | 에러 분석 보고서 | 5-10분 |
| 9 | pg_transform.py | 에러 분석 결과 | 수정된 SQL | 10-30분 |

## 3. 환경 설정 {#environment}

### 시스템 요구사항

- **운영체제:** Linux (CentOS, Ubuntu, Amazon Linux)
- **Python:** 3.7 이상
- **메모리:** 최소 4GB, 권장 8GB
- **디스크:** 최소 10GB 여유 공간

### oma.properties 기반 환경 변수

모든 경로 설정은 `/home/ec2-user/workspace/oma/config/oma.properties` 파일을 기반으로 합니다.

#### 기본 디렉토리 구조

```bash
# 기본 경로
OMA_BASE_DIR=/home/ec2-user/workspace/oma
APPLICATION_NAME=your-application

# 주요 디렉토리
TEST_FOLDER=${OMA_BASE_DIR}/${APPLICATION_NAME}/test
TEST_LOGS_FOLDER=${OMA_BASE_DIR}/${APPLICATION_NAME}/logs/test
APP_TRANSFORM_FOLDER=${OMA_BASE_DIR}/${APPLICATION_NAME}/application/transform
```

### 실제 디렉토리 경로

| 용도 | 환경 변수 | 실제 경로 | 설명 |
|------|-----------|-----------|------|
| 테스트 작업 디렉토리 | TEST_FOLDER | /home/ec2-user/workspace/oma/your-application/test | 모든 테스트 파일이 저장되는 기본 디렉토리 |
| 테스트 로그 디렉토리 | TEST_LOGS_FOLDER | /home/ec2-user/workspace/oma/your-application/logs/test | 테스트 실행 로그 저장 |
| XML 리스트 디렉토리 | APP_TRANSFORM_FOLDER | /home/ec2-user/workspace/oma/your-application/application/transform | XML 파일 목록 검색 |
| 프로그램 실행 디렉토리 | - | /home/ec2-user/workspace/oma/bin/test | 모든 Python 프로그램 위치 |

### 데이터베이스 연결 설정 (oma.properties 기반)

#### Oracle 연결 정보

```bash
export ORACLE_SVC_USER=your_oracle_user
export ORACLE_SVC_PASSWORD=your_oracle_password
export ORACLE_HOST=your-oracle-host
export ORACLE_PORT=1521
export ORACLE_SVC_CONNECT_STRING=your_service_name
export ORACLE_SID=your_sid
```

#### PostgreSQL 연결 정보

```bash
export PGHOST=your-postgres-host
export PGPORT=5432
export PGDATABASE=your_database
export PGUSER=your_postgres_user
export PGPASSWORD=your_postgres_password
```

### 테스트 디렉토리 구조

```
/home/ec2-user/workspace/oma/your-application/test/
├── xmllist/                    # XML 파일 목록
│   ├── src.list               # 소스 XML 파일 목록
│   ├── tgt.list               # 타겟 XML 파일 목록
│   └── postgres.list          # PostgreSQL용 XML 파일 목록
├── src_sql_extract/           # 소스에서 추출된 SQL 파일들
├── tgt_sql_extract/           # 타겟에서 추출된 SQL 파일들
├── orcl_sql_extract/          # Oracle용 SQL 파일들 (레거시)
├── pg_sql_extract/            # PostgreSQL용 SQL 파일들 (레거시)
├── dictionary/                # 데이터베이스 딕셔너리
│   └── all_dictionary.json   # 완전한 DB 딕셔너리
├── sampler/                   # 바인드 변수 샘플 JSON 파일들
├── src_sql_done/              # 소스 바인드 변수 치환 완료 SQL
├── tgt_sql_done/              # 타겟 바인드 변수 치환 완료 SQL
├── orcl_sql_done/             # Oracle 바인드 변수 치환 완료 SQL (레거시)
├── pg_sql_done/               # PostgreSQL 바인드 변수 치환 완료 SQL (레거시)
├── sqllist/                   # SQL 목록 관리 파일들
└── sql_results/               # SQL 실행 결과 저장
    ├── oracle/                # Oracle 실행 결과
    ├── postgres/              # PostgreSQL 실행 결과
    ├── comparison/            # 비교 결과
    ├── errors/                # 에러 분석 결과
    ├── reports/               # 보고서
    └── backup/                # 백업 파일들
```

### 선택적 환경 변수

| 환경 변수 | 설명 | 기본값 | 권장값 |
|-----------|------|--------|--------|
| SOURCE_DBMS_TYPE | 소스 데이터베이스 타입 | orcl | orcl (Oracle) |
| TARGET_DBMS_TYPE | 타겟 데이터베이스 타입 | postgres | postgres |
| SQL_BATCH_SIZE | SQL 배치 처리 크기 | 100 | 50-200 |
| SQL_PARALLEL_EXECUTION | 병렬 실행 여부 | true | true |
| SQL_MAX_WORKERS | 최대 워커 수 | 4 | CPU 코어 수 |
| SQL_TEMP_CLEANUP | 임시 파일 자동 정리 | true | true |
| SQL_ARCHIVE_DAYS | 로그 보관 일수 | 30 | 7-90 |

### 환경 설정 스크립트

#### 환경 변수 자동 설정

```bash
# 환경 변수 자동 설정
source /home/ec2-user/workspace/oma/bin/oma_env_your-application.sh

# 설정 확인
env | grep -E "(OMA|TEST|ORACLE|PG)" | sort
```

## 4. initTest.sh - 메인 실행 스크립트 {#init-script}

### 개요

전체 테스트 프로세스를 자동화하는 메인 스크립트로, 모든 개별 프로그램들을 순차적으로 실행하고 환경을 관리합니다.

### 상세 기능

- **환경 검증:** 필수 환경 변수 존재 여부 확인
- **데이터베이스 초기화:** sqllist 테이블 TRUNCATE
- **작업 공간 정리:** 이전 실행 결과 정리
- **프로그램 체인 실행:** 각 단계별 프로그램 순차 실행
- **에러 처리:** 각 단계별 성공/실패 확인
- **조건부 실행:** 타겟 DBMS에 따른 선택적 실행

### 실행 단계 상세

1. **환경 변수 확인:**
   - TARGET_DBMS_TYPE 설정 확인 (기본값: postgres)
   - 데이터베이스별 연결 정보 검증
   - 작업 디렉토리 존재 여부 확인

2. **sqllist 테이블 초기화:**
   - PostgreSQL: psql 명령어 사용
   - MySQL: mysql 명령어 사용
   - 연결 실패 시 즉시 종료

3. **작업 폴더 초기화:**
   - TEST_FOLDER 내 모든 파일 삭제
   - 디렉토리 구조 재생성

4. **프로그램 순차 실행:**
   - 각 프로그램 실행 전후 상태 메시지 출력
   - 실행 실패 시 에러 메시지와 함께 종료

### 사용법

```bash
# 실행 권한 부여
chmod +x initTest.sh

# 기본 실행
./initTest.sh

# 환경 변수와 함께 실행
TARGET_DBMS_TYPE=postgres TEST_FOLDER=/tmp/test ./initTest.sh
```

### 출력 예시

```
타겟 DBMS 타입: postgres
sqllist 테이블 초기화 중...
PostgreSQL에서 sqllist 테이블 초기화...
PostgreSQL sqllist 테이블 초기화 완료
작업 폴더 초기화 중...
작업 폴더 초기화 완료: /home/user/test
XML 파일에서 SQL 구문 추출 중...
딕셔너리 파일 생성 중...
바인드 변수 샘플링 중...
바인드 변수 매핑 중...
SQL 파일을 sqllist 테이블에 저장 중...
SQL 실행 및 결과 비교 중...
PostgreSQL 에러 유형 분석 중...
initTest.sh 실행 완료
```

**주의사항:**
- 실행 전 모든 환경 변수가 올바르게 설정되어 있는지 확인
- 데이터베이스 연결이 가능한지 사전 테스트
- 충분한 디스크 공간 확보
- 실행 중 중단하지 말고 완료까지 대기

## 5. 개별 프로그램 상세 {#programs}

### 5.1 XMLToSQL.py {#xmltosql}

#### 개요

MyBatis XML 매퍼 파일에서 SQL 구문을 추출하여 개별 SQL 파일로 저장하는 프로그램입니다.

#### 입출력 디렉토리 및 파일 흐름

##### 📁 입력 디렉토리

- **XML 리스트 파일:** `${APP_TRANSFORM_FOLDER}/xmllist/`
  - `src.list` - 소스 XML 파일 목록
  - `tgt.list` - 타겟 XML 파일 목록
  - `postgres.list` - PostgreSQL용 XML 파일 목록
- **실제 경로:** `/home/ec2-user/workspace/oma/your-application/test/xmllist/`

##### 📁 출력 디렉토리

- **소스 SQL 추출:** `${TEST_FOLDER}/src_sql_extract/`
  - 실제 경로: `/home/ec2-user/workspace/oma/your-application/test/src_sql_extract/`
  - 파일 형식: `[매퍼네임스페이스].[SQL ID].sql`
- **타겟 SQL 추출:** `${TEST_FOLDER}/tgt_sql_extract/`
  - 실제 경로: `/home/ec2-user/workspace/oma/your-application/test/tgt_sql_extract/`
  - 파일 형식: `[매퍼네임스페이스].[SQL ID].sql`

##### 🔄 처리 흐름

1. **XML 리스트 읽기:** xmllist/ 디렉토리에서 .list 파일들 스캔
2. **XML 파일 파싱:** 각 XML 파일에서 SQL 태그 추출
3. **SQL 구문 분리:** select, insert, update, delete 태그별 처리
4. **개별 파일 생성:** 각 SQL을 별도 .sql 파일로 저장

#### 상세 기능

- **XML 파일 관리:**
  - xmllist 디렉토리에서 XML 파일 목록 읽기
  - src, tgt, 또는 특정 DBMS 타입별 처리
  - 파일 존재 여부 및 접근 권한 확인

- **SQL 구문 추출:**
  - select, insert, update, delete 태그 처리
  - 중첩된 SQL 구문 처리
  - CDATA 섹션 내용 추출
  - 동적 SQL 태그 처리 (if, choose, foreach 등)

- **파일 생성:**
  - 각 SQL 구문을 개별 파일로 저장
  - 파일명: [매퍼ID].[SQL ID].sql
  - 디렉토리 구조: src_sql_extract/, tgt_sql_extract/

- **에러 처리:**
  - XML 파싱 에러 처리
  - 파일 I/O 에러 처리
  - 상세한 에러 로깅

#### 입력 파일 형식

##### XML 리스트 파일 예시 (xmllist/src.list)

```
/sorc001/src/src/main/resources/sqlmap/batch/BatchJobDao.xml
/sorc001/src/src/main/resources/sqlmap/batch/BatchJobRegDao.xml
/sorc001/src/src/main/resources/sqlmap/common/CommonDao.xml
/sorc001/src/src/main/resources/sqlmap/csr/CsrBatchJobDao.xml
```

##### MyBatis XML 매퍼 예시

```xml
<?xml version="1.0" encoding="UTF-8"?>
<mapper namespace="com.ourhome.itsm.batch.BatchJobDao">
    <select id="selectBatchJob" resultType="BatchJob">
        SELECT job_id, job_name, job_status, create_date
        FROM batch_jobs
        WHERE job_id = #{jobId}
          AND job_status = #{status}
    </select>
    
    <insert id="insertBatchJob">
        INSERT INTO batch_jobs (job_name, job_status, create_date)
        VALUES (#{jobName}, #{status}, SYSDATE)
    </insert>
</mapper>
```

#### 출력 파일 구조

```
# 소스 SQL 추출 결과
/home/ec2-user/workspace/oma/your-application/test/src_sql_extract/
├── com.example.batch.BatchJobDao.selectBatchJob.sql
├── com.example.batch.BatchJobDao.insertBatchJob.sql
├── com.example.common.CommonDao.selectCommonCode.sql
└── ...

# 타겟 SQL 추출 결과
/home/ec2-user/workspace/oma/your-application/test/tgt_sql_extract/
├── com.example.batch.BatchJobDao.selectBatchJob.sql
├── com.example.batch.BatchJobDao.insertBatchJob.sql
├── com.example.common.CommonDao.selectCommonCode.sql
└── ...
```

#### 생성되는 SQL 파일 예시

##### com.example.batch.BatchJobDao.selectBatchJob.sql

```sql
SELECT job_id, job_name, job_status, create_date
FROM batch_jobs
WHERE job_id = #{jobId}
  AND job_status = #{status}
```

#### 사용법

```bash
# 모든 XML 파일 처리 (src와 tgt 모두)
./XMLToSQL.py

# 소스 XML만 처리
./XMLToSQL.py src

# 타겟 XML만 처리
./XMLToSQL.py tgt

# 특정 DBMS 타입 처리
./XMLToSQL.py postgres
```

#### 로그 출력 예시

```
============================================================
환경 변수 확인 중...
============================================================
✅ SOURCE_DBMS_TYPE: orcl
✅ TARGET_DBMS_TYPE: postgres
✅ TEST_FOLDER: /home/ec2-user/workspace/oma/your-application/test
✅ APP_TRANSFORM_FOLDER: /home/ec2-user/workspace/oma/your-application/application/transform
============================================================
XML 파일 목록 처리 중...
============================================================
📁 처리할 XML 리스트 파일들: ['src', 'tgt']
📄 src.list 파일 처리 중... (125개 파일)
  - BatchJobDao.xml: 15개 SQL 추출
  - CommonDao.xml: 8개 SQL 추출
  - UserDao.xml: 12개 SQL 추출
✅ XML 파일 추출 완료: 1,247개 SQL 구문
📄 tgt.list 파일 처리 중... (125개 파일)
✅ XML 파일 추출 완료: 1,247개 SQL 구문
============================================================
XML 파일 처리 완료
============================================================
```

#### 성능 최적화

- **메모리 관리:** 대용량 XML 파일 스트리밍 처리
- **병렬 처리:** 여러 XML 파일 동시 처리
- **캐싱:** 중복 XML 파일 처리 방지
- **압축:** 출력 파일 압축 옵션

#### 주요 처리 통계

| 항목 | 일반적인 프로젝트 기준 | 설명 |
|------|----------------------|------|
| 처리된 XML 파일 수 | 100-200개 | src.list와 tgt.list 각각 |
| 추출된 SQL 구문 수 | 1,000-2,000개 | 각 디렉토리별 |
| 평균 처리 시간 | 5-10분 | 시스템 성능에 따라 변동 |
| 생성되는 파일 크기 | 평균 2KB | SQL 복잡도에 따라 변동 |

### 5.2 GetDictionary.py {#getdictionary}

#### 개요

Oracle 데이터베이스의 메타데이터와 샘플 데이터를 추출하여 포괄적인 딕셔너리를 생성하는 프로그램입니다.

#### 입출력 디렉토리 및 파일 흐름

##### 📥 입력 소스

- **Oracle 데이터베이스:** 
  - 호스트: `your-oracle-host:1521`
  - 사용자: `APP_USER, INF_USER, OWN_USER` 등 (프로젝트별 스키마)
  - 접근 방법: SQLPlus를 통한 메타데이터 쿼리
- **시스템 뷰:**
  - `USER_TABLES` - 테이블 목록
  - `USER_TAB_COLUMNS` - 컬럼 정보
  - `USER_CONSTRAINTS` - 제약조건
  - `USER_INDEXES` - 인덱스 정보

##### 📁 출력 디렉토리

- **딕셔너리 저장:** `${TEST_FOLDER}/dictionary/`
  - 실제 경로: `/home/ec2-user/workspace/oma/your-application/test/dictionary/`
  - 주요 파일: `all_dictionary.json`
- **로그 저장:** `${TEST_LOGS_FOLDER}/`
  - 실제 경로: `/home/ec2-user/workspace/oma/your-application/logs/test/`
  - 로그 파일: `dictionary_extraction.log`

##### 🔄 처리 흐름

1. **Oracle 연결:** SQLPlus를 통한 데이터베이스 연결
2. **스키마 스캔:** 여러 사용자 스키마의 모든 테이블 조회
3. **메타데이터 수집:** 각 테이블의 컬럼 정보 추출
4. **샘플 데이터 추출:** 각 컬럼별 실제 데이터 샘플링
5. **분류 및 분석:** 컬럼명 패턴 기반 데이터 타입 분류
6. **JSON 생성:** 구조화된 딕셔너리 파일 생성

#### 상세 기능

- **메타데이터 수집:**
  - 여러 스키마의 모든 사용자 테이블 목록 추출
  - 각 테이블의 컬럼 정보 수집
  - 데이터 타입, 길이, NULL 허용 여부
  - 기본값, 제약조건 정보

- **샘플 데이터 수집:**
  - 각 컬럼별 실제 데이터 샘플링 (기본 5개)
  - NULL 값 비율 계산
  - 데이터 분포 분석
  - 유니크 값 개수 계산

- **컬럼 분류:**
  - 네이밍 패턴 기반 분류 (ID, DATE, FLAG 등)
  - 데이터 타입 기반 분류
  - 비즈니스 의미 추론

- **딕셔너리 생성:**
  - JSON 형태로 구조화된 딕셔너리 생성
  - 검색 최적화를 위한 인덱스 생성
  - 압축 및 백업

#### 딕셔너리 구조

##### all_dictionary.json 구조 예시

```json
{
  "tables": {
    "BATCH_JOBS": {
      "schema": "APP_USER",
      "columns": {
        "JOB_ID": {
          "data_type": "NUMBER",
          "data_length": 10,
          "nullable": "N",
          "sample_values": [1001, 1002, 1003, 1004, 1005],
          "null_ratio": 0.0,
          "unique_count": 1250,
          "classification": "ID"
        },
        "JOB_NAME": {
          "data_type": "VARCHAR2",
          "data_length": 100,
          "nullable": "Y",
          "sample_values": ["배치작업1", "데이터동기화", "로그정리", "통계생성", "백업작업"],
          "null_ratio": 0.02,
          "unique_count": 1180,
          "classification": "NAME"
        },
        "CREATE_DATE": {
          "data_type": "DATE",
          "nullable": "N",
          "sample_values": ["2023-01-15", "2023-02-20", "2023-03-10"],
          "null_ratio": 0.0,
          "unique_count": 365,
          "classification": "DATE"
        },
        "JOB_STATUS": {
          "data_type": "VARCHAR2",
          "data_length": 10,
          "nullable": "N",
          "sample_values": ["RUNNING", "COMPLETED", "FAILED", "PENDING"],
          "null_ratio": 0.0,
          "unique_count": 4,
          "classification": "CODE"
        }
      },
      "row_count": 1250,
      "last_analyzed": "2025-07-29"
    }
  },
  "statistics": {
    "total_schemas": 5,
    "total_tables": 150,
    "total_columns": 2500,
    "extraction_time": "2025-07-29 12:00:00",
    "database_version": "Oracle 19c",
    "schemas_processed": [
      "APP_USER", "INF_USER", "OWN_USER", 
      "BATCH_USER", "REPORT_USER"
    ]
  }
}
```

#### 컬럼 분류 규칙

| 분류 | 패턴 | 실제 예시 | 개수 |
|------|------|-----------|------|
| ID | *_ID, *_NO, *_SEQ | JOB_ID, USER_NO, BATCH_SEQ | ~350개 |
| DATE | *_DATE, *_TIME, *_DT | CREATE_DATE, UPDATE_TIME, REG_DT | ~400개 |
| FLAG | *_FLAG, *_YN, USE_* | DELETE_FLAG, USE_YN, ACTIVE_FLAG | ~200개 |
| NAME | *_NAME, *_NM, *_TITLE | JOB_NAME, USER_NM, TITLE | ~300개 |
| CODE | *_CODE, *_CD, *_TYPE | STATUS_CODE, DEPT_CD, JOB_TYPE | ~250개 |
| AMOUNT | *_AMOUNT, *_CNT, *_SIZE | TOTAL_AMOUNT, RECORD_CNT, FILE_SIZE | ~100개 |

#### 실행 과정

1. **데이터베이스 연결:** Oracle SQLPlus를 통한 연결 (여러 스키마)
2. **테이블 목록 추출:** USER_TABLES 뷰에서 테이블 조회
3. **컬럼 정보 수집:** USER_TAB_COLUMNS 뷰에서 컬럼 조회
4. **샘플 데이터 추출:** 각 테이블별 SAMPLE(5) 쿼리 실행
5. **통계 정보 계산:** 데이터 분포 및 특성 분석
6. **딕셔너리 생성:** JSON 형태로 결과 저장

#### 사용법

```bash
# 기본 실행 (모든 스키마 처리)
./GetDictionary.py

# 특정 스키마만 처리
ORACLE_SCHEMA=APP_USER ./GetDictionary.py

# 샘플 크기 조정
SAMPLE_SIZE=10 ./GetDictionary.py

# 상세 로그 모드
DEBUG_MODE=true ./GetDictionary.py
```

#### 출력 파일

- **all_dictionary.json:** 완전한 데이터베이스 딕셔너리 (약 10-20MB)
- **dictionary_extraction.log:** 추출 과정 상세 로그
- **table_statistics.csv:** 테이블별 통계 정보
- **column_analysis.csv:** 컬럼별 분석 결과
- **schema_summary.json:** 스키마별 요약 정보

#### 성능 고려사항

- **대용량 테이블:** 100만 건 이상 테이블은 샘플링 비율 0.1%
- **네트워크 지연:** 배치 처리로 쿼리 최적화 (50개 테이블씩)
- **메모리 사용량:** 스트리밍 처리로 메모리 절약 (최대 2GB)
- **실행 시간:** 전체 처리 시간 약 10-30분

#### 실행 결과 예시

```
============================================================
Oracle 딕셔너리 추출 시작
============================================================
📊 대상 스키마: APP_USER, INF_USER, OWN_USER, BATCH_USER, REPORT_USER
🔗 Oracle 연결: your-oracle-host:1521/your_service

📋 스키마별 테이블 수집 중...
  - APP_USER: 45개 테이블, 850개 컬럼
  - INF_USER: 25개 테이블, 420개 컬럼
  - OWN_USER: 30개 테이블, 580개 컬럼
  - BATCH_USER: 20개 테이블, 350개 컬럼
  - REPORT_USER: 15개 테이블, 200개 컬럼

🔍 샘플 데이터 수집 중...
  - 처리된 테이블: 135개
  - 수집된 샘플: 10,000개
  - 분류된 컬럼: 2,400개

💾 딕셔너리 파일 생성 중...
  - 파일 크기: 12.5MB
  - 저장 위치: /home/ec2-user/workspace/oma/your-application/test/dictionary/all_dictionary.json

✅ 딕셔너리 추출 완료 (소요시간: 15분 20초)
============================================================
```

### 5.3 BindSampler.py {#bindsampler}

#### 개요

SQL 파일의 바인드 변수를 분석하고 데이터베이스 딕셔너리를 기반으로 적절한 샘플 값을 할당하는 프로그램입니다.

#### 입출력 디렉토리 및 파일 흐름

##### 📥 입력 소스

- **SQL 파일들:** `${TEST_FOLDER}/src_sql_extract/`
  - 실제 경로: `/home/ec2-user/workspace/oma/your-application/test/src_sql_extract/`
  - 파일 개수: 약 1,000-2,000개 .sql 파일
  - 파일 형식: `[매퍼네임스페이스].[SQL ID].sql`
- **데이터베이스 딕셔너리:** `${TEST_FOLDER}/dictionary/all_dictionary.json`
  - 실제 경로: `/home/ec2-user/workspace/oma/your-application/test/dictionary/all_dictionary.json`
  - 파일 크기: 약 10-20MB
  - 포함 정보: 컬럼의 메타데이터 및 샘플 데이터

##### 📁 출력 디렉토리

- **바인드 변수 샘플:** `${TEST_FOLDER}/sampler/`
  - 실제 경로: `/home/ec2-user/workspace/oma/your-application/test/sampler/`
  - 파일 형식: `[매퍼네임스페이스].[SQL ID].json`
  - 생성 파일 수: SQL 파일 수와 동일
- **로그 파일:** `${TEST_LOGS_FOLDER}/bind_sampling.log`
  - 실제 경로: `/home/ec2-user/workspace/oma/your-application/logs/test/bind_sampling.log`

##### 🔄 처리 흐름

1. **딕셔너리 로드:** all_dictionary.json 파일을 메모리에 로드
2. **SQL 파일 스캔:** src_sql_extract/ 디렉토리의 모든 .sql 파일 처리
3. **바인드 변수 추출:** #{variable}, ${variable}, :variable 패턴 검색
4. **데이터 타입 추정:** 변수명 패턴과 딕셔너리 매칭
5. **샘플 값 할당:** 딕셔너리에서 적절한 샘플 값 선택
6. **JSON 파일 생성:** 각 SQL별 바인드 변수 정보를 JSON으로 저장

#### 상세 기능

- **바인드 변수 추출:**
  - MyBatis 형식: #{variable}, ${variable}
  - Oracle 형식: :variable
  - 중첩된 바인드 변수 처리
  - 조건부 바인드 변수 처리

- **데이터 타입 추정:**
  - 변수명 패턴 분석 (date_*, *_id, is_* 등)
  - 딕셔너리 컬럼명 매칭
  - SQL 컨텍스트 분석
  - 기본 데이터 타입 추론

- **샘플 값 할당:**
  - 딕셔너리에서 직접 매칭
  - 유사 컬럼명 검색
  - 동일 데이터 타입 컬럼에서 선택
  - 기본값 할당

- **결과 저장:**
  - SQL 파일별 JSON 파일 생성
  - 바인드 변수별 메타데이터 저장
  - 매칭 신뢰도 점수 계산

#### 바인드 변수 패턴 인식

| 패턴 | 추정 타입 | 샘플 값 | 설명 |
|------|-----------|---------|------|
| *_id, *_no, *_seq | NUMBER | 1, 2, 3... | 식별자 컬럼 |
| *_date, *_time, *_dt | DATE | SYSDATE, '2023-01-01' | 날짜/시간 컬럼 |
| is_*, *_flag, *_yn | VARCHAR2(1) | 'Y', 'N' | 플래그 컬럼 |
| *_name, *_nm | VARCHAR2 | '테스트명' | 이름 컬럼 |
| *_code, *_cd | VARCHAR2 | '001', 'A01' | 코드 컬럼 |
| *_amount, *_price | NUMBER | 1000, 50000 | 금액 컬럼 |

#### 샘플링 알고리즘

1. **직접 매칭:** 변수명이 딕셔너리 컬럼명과 정확히 일치
2. **패턴 매칭:** 변수명 패턴으로 데이터 타입 추정
3. **유사도 매칭:** 편집 거리 기반 유사 컬럼 검색
4. **타입 매칭:** 동일 데이터 타입 컬럼에서 랜덤 선택
5. **기본값 할당:** 모든 매칭 실패 시 기본값 사용

#### 출력 JSON 구조

##### sampler/UserMapper.selectUser.json 예시

```json
{
  "sql_file": "UserMapper.selectUser.sql",
  "bind_variables": {
    "userId": {
      "data_type": "NUMBER",
      "sample_value": 12345,
      "confidence": 0.95,
      "match_type": "direct",
      "source_column": "USERS.USER_ID",
      "alternatives": [67890, 11111, 22222]
    },
    "userName": {
      "data_type": "VARCHAR2",
      "sample_value": "김철수",
      "confidence": 0.80,
      "match_type": "pattern",
      "source_column": "USERS.USER_NAME",
      "alternatives": ["이영희", "박민수", "최영수"]
    },
    "startDate": {
      "data_type": "DATE",
      "sample_value": "2023-01-01",
      "confidence": 0.70,
      "match_type": "type",
      "source_column": "ORDERS.ORDER_DATE",
      "alternatives": ["2023-01-02", "2023-01-03"]
    }
  },
  "statistics": {
    "total_variables": 3,
    "direct_matches": 1,
    "pattern_matches": 1,
    "type_matches": 1,
    "default_values": 0,
    "average_confidence": 0.82
  },
  "processing_time": 0.15,
  "timestamp": "2025-07-29 12:30:00"
}
```

#### 사용법

```bash
# 기본 실행 (모든 SQL 파일 처리)
./BindSampler.py

# 특정 디렉토리 처리
SQL_EXTRACT_DIR=/path/to/sql ./BindSampler.py

# 샘플 개수 조정
SAMPLE_COUNT=10 ./BindSampler.py

# 신뢰도 임계값 설정
CONFIDENCE_THRESHOLD=0.7 ./BindSampler.py
```

#### 성능 최적화

- **딕셔너리 캐싱:** 메모리에 딕셔너리 로드
- **병렬 처리:** 여러 SQL 파일 동시 처리
- **인덱싱:** 컬럼명 검색 최적화
- **배치 처리:** 유사한 패턴 그룹화

### 5.4 BindMapper.py {#bindmapper}

#### 개요

BindSampler의 결과를 사용하여 SQL 파일의 바인드 변수를 실제 값으로 치환하여 실행 가능한 SQL을 생성하는 프로그램입니다.

### 5.5 SaveSQLToDB.py {#savesqltodb}

#### 개요

바인드 변수가 치환된 SQL 파일들을 체계적으로 관리하기 위해 sqllist 테이블에 저장하는 프로그램입니다.

### 5.6 ExecuteAndCompareSQL.py {#executecompare}

#### 개요

sqllist 테이블의 SQL을 Oracle과 PostgreSQL에서 실행하고 결과를 비교하여 호환성을 검증하는 핵심 프로그램입니다.

### 5.7 analyze_pg_errors.py {#analyze-errors}

#### 개요

PostgreSQL 실행 중 발생한 에러를 체계적으로 분석하고 분류하여 수정 방향을 제시하는 프로그램입니다.

### 5.8 pg_transform.py {#pg-transform}

#### 개요

analyze_pg_errors.py의 분석 결과를 바탕으로 PostgreSQL 호환성을 위한 SQL 구문 자동 수정을 수행하는 프로그램입니다.

## 6. 문제 해결 {#troubleshooting}

### 일반적인 문제들

#### 환경 변수 관련

**문제:** 환경 변수가 설정되지 않음

**해결:**

```bash
# 환경 변수 확인
env | grep -E "(ORACLE|PG|MYSQL|TEST)"

# 필수 환경 변수 설정
export TARGET_DBMS_TYPE=postgres
export PGHOST=your-host
export PGUSER=your-user
export PGPASSWORD=your-password
```

#### 데이터베이스 연결 문제

**문제:** 데이터베이스 연결 실패

**해결:**

```bash
# PostgreSQL 연결 테스트
psql -h $PGHOST -p $PGPORT -U $PGUSER -d $PGDATABASE -c "SELECT 1"

# Oracle 연결 테스트
sqlplus $ORACLE_SVC_USER/$ORACLE_SVC_PASSWORD@$ORACLE_SVC_CONNECT_STRING
```

#### 권한 문제

**문제:** 스크립트 실행 권한 없음

**해결:**

```bash
# 실행 권한 부여
chmod +x *.py *.sh

# 소유자 확인
ls -la *.py *.sh
```

#### 메모리 부족

**문제:** 대용량 데이터 처리 시 메모리 부족

**해결:**

```bash
# 배치 크기 줄이기
export SQL_BATCH_SIZE=10

# 워커 수 줄이기
export SQL_MAX_WORKERS=2

# 메모리 사용량 모니터링
top -p $(pgrep -f python3)
```

## 7. 모범 사례 {#best-practices}

### 실행 전 준비사항

- [ ] 모든 환경 변수 설정 확인
- [ ] 데이터베이스 연결 테스트
- [ ] 충분한 디스크 공간 확보 (최소 10GB)
- [ ] 백업 디렉토리 준비
- [ ] 로그 디렉토리 권한 확인

### 실행 중 모니터링

- [ ] 로그 파일 실시간 모니터링
- [ ] 시스템 리소스 사용량 확인
- [ ] 데이터베이스 연결 상태 점검
- [ ] 에러 발생 시 즉시 대응

### 실행 후 검증

- [ ] 모든 출력 파일 생성 확인
- [ ] 에러 로그 상세 검토
- [ ] 통계 정보 분석
- [ ] 결과 데이터 샘플 검증
- [ ] 성능 지표 분석

### 성능 최적화 팁

- **배치 크기 조정:** 시스템 리소스에 맞게 SQL_BATCH_SIZE 설정
- **병렬 처리:** CPU 코어 수에 맞게 SQL_MAX_WORKERS 설정
- **메모리 관리:** 대용량 결과셋은 스트리밍 처리
- **네트워크 최적화:** 데이터베이스와 가까운 위치에서 실행
- **임시 파일 관리:** 정기적인 임시 파일 정리

## 8. 부록 {#appendix}

### 주요 디렉토리 구조

```
test/
├── xmllist/                    # XML 파일 목록
│   ├── src.list
│   └── tgt.list
├── src_sql_extract/           # 추출된 소스 SQL
├── tgt_sql_extract/           # 추출된 타겟 SQL
├── dictionary/                # 데이터베이스 딕셔너리
│   └── all_dictionary.json
├── sampler/                   # 바인드 변수 샘플
├── bind_mapped/               # 바인드 변수 치환된 SQL
├── logs/                      # 로그 파일들
└── backup/                    # 백업 파일들
```

### 주요 로그 파일

- **xmltosql.log:** XML 파싱 및 SQL 추출 로그
- **dictionary_extraction.log:** 딕셔너리 생성 로그
- **bind_sampling.log:** 바인드 변수 샘플링 로그
- **sql_execution.log:** SQL 실행 로그
- **error_analysis.log:** 에러 분석 로그

### 유용한 명령어

```bash
# 전체 진행 상황 확인
tail -f *.log | grep -E "(완료|ERROR|성공|실패)"

# 에러만 필터링
grep -i error *.log

# 특정 프로그램 로그만 확인
grep "XMLToSQL" *.log

# 실행 시간 분석
grep "실행 시간" *.log | sort -k3 -n

# 디스크 사용량 확인
du -sh */
```
