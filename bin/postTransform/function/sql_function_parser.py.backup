#!/usr/bin/env python3
"""
SQL í•¨ìˆ˜ íŒŒì„œ - ë³µì¡í•œ ì¤‘ì²© êµ¬ì¡° ì •í™• ì²˜ë¦¬
Version: 1.0
Author: Amazon Q Developer
Date: 2025-07-19
"""

import re
from enum import Enum
from dataclasses import dataclass
from typing import List, Optional, Union

class TokenType(Enum):
    FUNCTION = "FUNCTION"
    KEYWORD = "KEYWORD"
    IDENTIFIER = "IDENTIFIER"
    STRING = "STRING"
    NUMBER = "NUMBER"
    OPERATOR = "OPERATOR"
    LPAREN = "LPAREN"
    RPAREN = "RPAREN"
    COMMA = "COMMA"
    WHITESPACE = "WHITESPACE"
    COMMENT = "COMMENT"
    EOF = "EOF"

@dataclass
class Token:
    type: TokenType
    value: str
    position: int

class SQLTokenizer:
    def __init__(self):
        # SQL í•¨ìˆ˜ëª… ë¦¬ìŠ¤íŠ¸ (MySQL, Oracle, PostgreSQL)
        self.functions = {
            'CONCAT', 'SUBSTRING', 'SUBSTR', 'UPPER', 'LOWER', 'TRIM', 'LTRIM', 'RTRIM',
            'REPLACE', 'LENGTH', 'CHAR_LENGTH', 'LEFT', 'RIGHT', 'REVERSE',
            'LOCATE', 'INSTR', 'POSITION', 'LPAD', 'RPAD', 'REPEAT', 'SPACE',
            'INITCAP', 'TRANSLATE', 'ASCII', 'CHR', 'SOUNDEX', 'REGEXP_REPLACE',
            'SUM', 'COUNT', 'AVG', 'MAX', 'MIN', 'ROUND', 'CEIL', 'CEILING', 'FLOOR',
            'GROUP_CONCAT',
            'ABS', 'MOD', 'POWER', 'SQRT', 'SIGN', 'GREATEST', 'LEAST',
            'DATE_FORMAT', 'STR_TO_DATE', 'DATE_ADD', 'DATE_SUB', 'DATEDIFF',
            'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'SECOND', 'DAYOFWEEK',
            'UNIX_TIMESTAMP', 'FROM_UNIXTIME', 'TIME_FORMAT', 'NOW', 'CURDATE',
            'IFNULL', 'ISNULL', 'COALESCE', 'NULLIF', 'NVL', 'NVL2',
            'CAST', 'CONVERT', 'TO_NUMBER', 'TO_CHAR', 'TO_DATE'
        }
        
        # SQL í‚¤ì›Œë“œ
        self.keywords = {
            'SELECT', 'FROM', 'WHERE', 'GROUP', 'ORDER', 'BY', 'HAVING',
            'CASE', 'WHEN', 'THEN', 'ELSE', 'END', 'AS', 'AND', 'OR', 'NOT',
            'IN', 'EXISTS', 'BETWEEN', 'LIKE', 'IS', 'NULL', 'TRUE', 'FALSE',
            'DISTINCT', 'ALL', 'INNER', 'LEFT', 'RIGHT', 'OUTER', 'JOIN', 'ON',
            'UNION', 'INTERSECT', 'EXCEPT', 'INTERVAL',
            'LEADING', 'TRAILING', 'BOTH'  # TRIM í•¨ìˆ˜ìš© í‚¤ì›Œë“œ ì¶”ê°€
        }
        
        # í† í° íŒ¨í„´ë“¤
        self.patterns = [
            (r'/\*.*?\*/', TokenType.COMMENT),           # /* ì£¼ì„ */
            (r'--.*?$', TokenType.COMMENT),              # -- ì£¼ì„
            (r"'(?:[^']|'')*'", TokenType.STRING),       # 'ë¬¸ìì—´'
            (r'"(?:[^"]|"")*"', TokenType.STRING),       # "ë¬¸ìì—´"
            (r'\d+\.?\d*', TokenType.NUMBER),            # ìˆ«ì
            (r'[A-Za-z_][A-Za-z0-9_]*', TokenType.IDENTIFIER),  # ì‹ë³„ì
            (r'\(', TokenType.LPAREN),                   # (
            (r'\)', TokenType.RPAREN),                   # )
            (r',', TokenType.COMMA),                     # ,
            (r'\.', TokenType.OPERATOR),                 # . (ì  ì—°ì‚°ì)
            (r'[+\-*/=<>!]+', TokenType.OPERATOR),       # ì—°ì‚°ì
            (r'\s+', TokenType.WHITESPACE),              # ê³µë°±
        ]
        
        self.compiled_patterns = [(re.compile(pattern, re.MULTILINE | re.DOTALL), token_type) 
                                 for pattern, token_type in self.patterns]
    
    def tokenize(self, text: str) -> List[Token]:
        """í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„ë¦¬"""
        tokens = []
        position = 0
        
        while position < len(text):
            matched = False
            
            for pattern, token_type in self.compiled_patterns:
                match = pattern.match(text, position)
                if match:
                    value = match.group(0)
                    
                    # í† í° íƒ€ì… ì„¸ë¶„í™”
                    if token_type == TokenType.IDENTIFIER:
                        upper_value = value.upper()
                        if upper_value in self.functions:
                            token_type = TokenType.FUNCTION
                        elif upper_value in self.keywords:
                            token_type = TokenType.KEYWORD
                    
                    # ê³µë°±ê³¼ ì£¼ì„ì€ ê±´ë„ˆë›°ê¸° (ì„ íƒì )
                    if token_type not in [TokenType.WHITESPACE, TokenType.COMMENT]:
                        tokens.append(Token(token_type, value, position))
                    
                    position = match.end()
                    matched = True
                    break
            
            if not matched:
                # ë§¤ì¹­ë˜ì§€ ì•Šì€ ë¬¸ìëŠ” ê±´ë„ˆë›°ê¸°
                position += 1
        
        tokens.append(Token(TokenType.EOF, '', position))
        return tokens

class SQLFunctionParser:
    def __init__(self):
        self.tokenizer = SQLTokenizer()
        self.tokens = []
        self.position = 0
        self.current_token = None
    
    def parse(self, text: str) -> List[str]:
        """SQL í…ìŠ¤íŠ¸ì—ì„œ ì™„ì „í•œ í•¨ìˆ˜ë“¤ì„ ì¶”ì¶œ"""
        self.tokens = self.tokenizer.tokenize(text)
        self.position = 0
        self.current_token = self.tokens[0] if self.tokens else None
        self.set_original_text(text)  # ì›ë³¸ í…ìŠ¤íŠ¸ ì„¤ì • ì¶”ê°€
        
        functions = []
        
        while not self.is_at_end():
            if self.current_token.type == TokenType.FUNCTION:
                func = self.parse_function()
                if func:
                    functions.append(func)
            elif self.current_token.type == TokenType.KEYWORD and self.current_token.value.upper() == 'CASE':
                case_stmt = self.parse_case_statement()
                if case_stmt:
                    functions.append(case_stmt)
            else:
                self.advance()
        
        return functions
    
    def advance(self):
        """ë‹¤ìŒ í† í°ìœ¼ë¡œ ì´ë™"""
        if not self.is_at_end():
            self.position += 1
            if self.position < len(self.tokens):
                self.current_token = self.tokens[self.position]
    
    def is_at_end(self):
        """í† í° ëì— ë„ë‹¬í–ˆëŠ”ì§€ í™•ì¸"""
        return self.current_token is None or self.current_token.type == TokenType.EOF
    
    def peek(self, offset=1):
        """ì•ì˜ í† í°ì„ ë¯¸ë¦¬ ë³´ê¸°"""
        peek_pos = self.position + offset
        if peek_pos < len(self.tokens):
            return self.tokens[peek_pos]
        return None
    
    def parse_function(self):
        """í•¨ìˆ˜ë¥¼ íŒŒì‹± (ì¤‘ì²© êµ¬ì¡° í¬í•¨) - TRIM, CAST í•¨ìˆ˜ íŠ¹ë³„ ì²˜ë¦¬"""
        if self.current_token.type != TokenType.FUNCTION:
            return None
        
        start_pos = self.current_token.position
        func_name = self.current_token.value.upper()
        self.advance()
        
        # í•¨ìˆ˜ ë’¤ì— ê´„í˜¸ê°€ ìˆëŠ”ì§€ í™•ì¸
        if self.current_token.type != TokenType.LPAREN:
            return None
        
        # TRIM í•¨ìˆ˜ íŠ¹ë³„ ì²˜ë¦¬
        if func_name == 'TRIM':
            return self.parse_trim_function(start_pos)
        
        # ëª¨ë“  í•¨ìˆ˜ë¥¼ ì¼ë°˜ í•¨ìˆ˜ë¡œ ì²˜ë¦¬ (CAST íŠ¹ë³„ ì²˜ë¦¬ ì œê±°)
        return self.parse_regular_function(start_pos)
    
    def parse_cast_function(self, start_pos):
        """CAST í•¨ìˆ˜ ì „ìš© íŒŒì‹± - AS ì ˆì„ ì •í™•íˆ êµ¬ë¶„"""
        paren_count = 0
        as_positions = []
        current_pos = self.position
        
        while not self.is_at_end():
            if self.current_token.type == TokenType.LPAREN:
                paren_count += 1
            elif self.current_token.type == TokenType.RPAREN:
                paren_count -= 1
                if paren_count == 0:
                    # CAST í•¨ìˆ˜ì˜ ë‹«ëŠ” ê´„í˜¸ ì°¾ìŒ
                    end_pos = self.current_token.position + len(self.current_token.value)
                    self.advance()
                    
                    # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ CAST í•¨ìˆ˜ ì¶”ì¶œ
                    original_text = self.get_original_text()
                    if start_pos < len(original_text) and end_pos <= len(original_text):
                        cast_function = original_text[start_pos:end_pos].strip()
                        
                        # CAST í•¨ìˆ˜ ë‚´ë¶€ êµ¬ì¡° ì •ë¦¬
                        return self.clean_cast_function(cast_function)
                    break
            elif (self.current_token.type == TokenType.KEYWORD and 
                  self.current_token.value.upper() == 'AS'):
                # AS ì ˆ ìœ„ì¹˜ ê¸°ë¡ (ê´„í˜¸ ë ˆë²¨ê³¼ í•¨ê»˜)
                as_positions.append((self.current_token.position, paren_count))
            
            self.advance()
        
        return None
    
    def clean_cast_function(self, cast_func):
        """CAST í•¨ìˆ˜ ë‚´ë¶€ êµ¬ì¡° ì •ë¦¬"""
        # CAST(expression AS datatype) í˜•íƒœë¡œ ì •ë¦¬
        
        # 1. CAST( ë¶€ë¶„ ì¶”ì¶œ
        cast_start = cast_func.find('(')
        if cast_start == -1:
            return cast_func
        
        inner_content = cast_func[cast_start + 1:-1]  # ê´„í˜¸ ë‚´ë¶€ ë‚´ìš©
        
        # 2. ê°€ì¥ ë°”ê¹¥ìª½ AS ì ˆ ì°¾ê¸°
        as_pos = self.find_outermost_as(inner_content)
        
        if as_pos == -1:
            # AS ì ˆì´ ì—†ìœ¼ë©´ ì¶”ê°€
            return f"CAST({inner_content} AS CHAR(255))"
        
        # 3. AS ì ˆ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
        expression = inner_content[:as_pos].strip()
        datatype = inner_content[as_pos + 2:].strip()  # AS ì´í›„ ë¶€ë¶„
        
        # 4. expression ë‚´ë¶€ì˜ ì˜ëª»ëœ AS ì ˆ ì œê±°
        expression = self.remove_inner_as_clauses(expression)
        
        return f"CAST({expression} AS {datatype})"
    
    def find_outermost_as(self, text):
        """ê°€ì¥ ë°”ê¹¥ìª½ AS ì ˆì˜ ìœ„ì¹˜ ì°¾ê¸°"""
        paren_count = 0
        i = 0
        
        while i < len(text) - 1:
            if text[i] == '(':
                paren_count += 1
            elif text[i] == ')':
                paren_count -= 1
            elif (paren_count == 0 and 
                  text[i:i+2].upper() == 'AS' and 
                  (i == 0 or not text[i-1].isalnum()) and
                  (i+2 >= len(text) or not text[i+2].isalnum())):
                return i
            i += 1
        
        return -1
    
    def remove_inner_as_clauses(self, expression):
        """í•¨ìˆ˜ ë‚´ë¶€ì˜ ì˜ëª»ëœ AS ì ˆ ì œê±°"""
        # SUBSTRING(col, pos, len AS datatype) â†’ SUBSTRING(col, pos, len)
        # í•¨ìˆ˜ ë‚´ë¶€ì˜ AS ì ˆë§Œ ì œê±°í•˜ê³  í•¨ìˆ˜ êµ¬ì¡°ëŠ” ìœ ì§€
        
        result = expression
        
        # í•¨ìˆ˜ ë‚´ë¶€ AS ì ˆ íŒ¨í„´ ì œê±°
        # 3ê°œ íŒŒë¼ë¯¸í„° í•¨ìˆ˜: func(a, b, c AS type) â†’ func(a, b, c)
        result = re.sub(r'(\w+\s*\([^)]*,\s*[^)]*,\s*[^)]+)\s+AS\s+[^)]+(\))', 
                       r'\1\2', result, flags=re.IGNORECASE)
        
        # 2ê°œ íŒŒë¼ë¯¸í„° í•¨ìˆ˜: func(a, b AS type) â†’ func(a, b)  
        result = re.sub(r'(\w+\s*\([^)]*,\s*[^)]+)\s+AS\s+[^)]+(\))', 
                       r'\1\2', result, flags=re.IGNORECASE)
        
        return result
    
    def parse_regular_function(self, start_pos):
        """ì¼ë°˜ í•¨ìˆ˜ íŒŒì‹± (ê¸°ì¡´ ë¡œì§)"""
        # í˜„ì¬ í† í°ì´ ì—¬ëŠ” ê´„í˜¸ì¸ì§€ í™•ì¸
        if self.current_token.type != TokenType.LPAREN:
            return None
            
        # ê´„í˜¸ ë§¤ì¹­ìœ¼ë¡œ í•¨ìˆ˜ ì „ì²´ ì¶”ì¶œ
        paren_count = 0
        end_pos = start_pos
        
        while not self.is_at_end():
            if self.current_token.type == TokenType.LPAREN:
                paren_count += 1
            elif self.current_token.type == TokenType.RPAREN:
                paren_count -= 1
                if paren_count == 0:
                    end_pos = self.current_token.position + len(self.current_token.value)
                    self.advance()
                    break
            
            self.advance()
        
        # AS ì ˆì€ í•¨ìˆ˜ ì¶”ì¶œì— í¬í•¨í•˜ì§€ ì•ŠìŒ (ë³„ë„ ì²˜ë¦¬)
        # AS ë³„ì¹­ì´ ìˆì–´ë„ í•¨ìˆ˜ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        
        # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ í•¨ìˆ˜ ì¶”ì¶œ
        original_text = self.get_original_text()
        if start_pos < len(original_text) and end_pos <= len(original_text):
            return original_text[start_pos:end_pos].strip()
        
        return None
    
    def parse_case_statement(self):
        """CASE ë¬¸ì„ íŒŒì‹± - ì›ë³¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ì¶œ"""
        if (self.current_token.type != TokenType.KEYWORD or 
            self.current_token.value.upper() != 'CASE'):
            return None
        
        # CASE ë¬¸ ì‹œì‘ ìœ„ì¹˜ ê¸°ë¡
        start_pos = self.current_token.position
        self._case_start_pos = start_pos
        
        case_count = 1
        tokens_in_case = [self.current_token]  # CASE í† í° í¬í•¨
        self.advance()
        
        # ë””ë²„ê·¸: í† í° ìˆ˜ì§‘ ê³¼ì • ë¡œê·¸
        debug_tokens = []
        
        while not self.is_at_end() and case_count > 0:
            if (self.current_token.type == TokenType.KEYWORD and 
                self.current_token.value.upper() == 'CASE'):
                case_count += 1
            elif (self.current_token.type == TokenType.KEYWORD and 
                  self.current_token.value.upper() == 'END'):
                case_count -= 1
                
            tokens_in_case.append(self.current_token)
            debug_tokens.append(f"{self.current_token.type.value}:{self.current_token.value}")
            
            if case_count == 0:
                # CASE ë¬¸ ë ìœ„ì¹˜ ê¸°ë¡
                end_pos = self.current_token.position + len(self.current_token.value)
                self._case_end_pos = end_pos
                
                self.advance()
                
                # AS ì ˆ í™•ì¸
                if (not self.is_at_end() and 
                    self.current_token.type == TokenType.KEYWORD and 
                    self.current_token.value.upper() == 'AS'):
                    
                    tokens_in_case.append(self.current_token)  # AS ì¶”ê°€
                    debug_tokens.append(f"{self.current_token.type.value}:{self.current_token.value}")
                    self.advance()  # AS ê±´ë„ˆë›°ê¸°
                    
                    # ë³„ì¹­ í™•ì¸
                    if (not self.is_at_end() and 
                        self.current_token.type in [TokenType.IDENTIFIER, TokenType.STRING]):
                        tokens_in_case.append(self.current_token)  # ë³„ì¹­ ì¶”ê°€
                        debug_tokens.append(f"{self.current_token.type.value}:{self.current_token.value}")
                        # AS ì ˆê¹Œì§€ í¬í•¨í•˜ì—¬ ë ìœ„ì¹˜ ì—…ë°ì´íŠ¸
                        self._case_end_pos = self.current_token.position + len(self.current_token.value)
                        self.advance()
                
                # ë””ë²„ê·¸ ë¡œê·¸ ì¶œë ¥
                print(f"ğŸ” CASE ë¬¸ í† í° ìˆ˜ì§‘: {len(tokens_in_case)}ê°œ")
                print(f"ğŸ” í† í° ëª©ë¡: {debug_tokens[:20]}...")  # ì²˜ìŒ 20ê°œë§Œ ì¶œë ¥
                
                # í† í°ë“¤ì„ ë‹¤ì‹œ ì¡°í•©í•´ì„œ CASE ë¬¸ ìƒì„± - ìˆœì°¨ì  ì²˜ë¦¬ë¡œ ëˆ„ë½ ë°©ì§€
                case_text = ""
                prev_token_type = None
                
                for i, token in enumerate(tokens_in_case):
                    # ê³µë°± í† í°ì€ ê±´ë„ˆë›°ë˜, í•„ìš”í•œ ê³³ì— ê³µë°± ì¶”ê°€
                    if token.type == TokenType.WHITESPACE:
                        continue
                    
                    # ì²« ë²ˆì§¸ í† í°
                    if i == 0 or not case_text:
                        case_text += token.value
                    else:
                        # í† í° ì‚¬ì´ì— ê³µë°±ì´ í•„ìš”í•œì§€ íŒë‹¨
                        need_space = True
                        
                        # ì  ì—°ì‚°ì ì²˜ë¦¬: ì•ë’¤ ê³µë°± ì—†ìŒ
                        if token.value == '.' or (prev_token_type == TokenType.OPERATOR and tokens_in_case[i-1].value == '.'):
                            need_space = False
                        # ì—¬ëŠ” ê´„í˜¸: ì•ì—ë§Œ ê³µë°±
                        elif token.value == '(':
                            need_space = True
                        # ë‹«ëŠ” ê´„í˜¸: ë’¤ì—ë§Œ ê³µë°± (ë‹¤ìŒ í† í°ì—ì„œ ì²˜ë¦¬)
                        elif token.value == ')':
                            need_space = False
                        # ì‰¼í‘œ: ë’¤ì—ë§Œ ê³µë°±
                        elif prev_token_type == TokenType.COMMA:
                            need_space = True
                        elif token.type == TokenType.COMMA:
                            need_space = False
                        
                        # ê³µë°± ì¶”ê°€
                        if need_space and not case_text.endswith(' '):
                            case_text += " "
                        
                        # í† í° ê°’ ì¶”ê°€
                        case_text += token.value
                        
                        # ì—°ì‚°ìë‚˜ ì‰¼í‘œ ë’¤ì—ëŠ” ê³µë°± ì¶”ê°€
                        if token.type in [TokenType.OPERATOR, TokenType.COMMA] and token.value != '.':
                            case_text += " "
                    
                    prev_token_type = token.type
                
                # ë””ë²„ê·¸: ìµœì¢… CASE ë¬¸ ê²°ê³¼ í™•ì¸
                print(f"ğŸ” CASE ë¬¸ ì¬ì¡°í•© ì™„ë£Œ: {len(case_text)}ì")
                print(f"ğŸ” CASE ë¬¸ ê²°ê³¼ ìƒ˜í”Œ: {case_text[:200]}...")
                
                # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì¶”ì¶œí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½ (í† í° ëˆ„ë½ ë°©ì§€)
                original_text = self.get_original_text()
                if original_text and hasattr(self, '_case_start_pos') and hasattr(self, '_case_end_pos'):
                    # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ CASE ë¬¸ ì§ì ‘ ì¶”ì¶œ
                    case_from_original = original_text[self._case_start_pos:self._case_end_pos].strip()
                    if case_from_original:
                        print(f"ğŸ” ì›ë³¸ì—ì„œ ì¶”ì¶œí•œ CASE ë¬¸: {case_from_original[:200]}...")
                        return case_from_original
                
                return case_text
            else:
                self.advance()
        
        return None
    
    def get_original_text(self):
        """ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜ (í† í¬ë‚˜ì´ì €ì—ì„œ ì„¤ì •)"""
        return getattr(self, '_original_text', '')
    
    def set_original_text(self, text):
        """ì›ë³¸ í…ìŠ¤íŠ¸ ì„¤ì •"""
        self._original_text = text
    
    def replace_subqueries_with_parser(self, text: str) -> str:
        """íŒŒì„œë¥¼ ì‚¬ìš©í•´ì„œ ì„œë¸Œì¿¼ë¦¬ë¥¼ 1ë¡œ ëŒ€ì²´"""
        self.tokens = self.tokenizer.tokenize(text)
        self.position = 0
        self.current_token = self.tokens[0] if self.tokens else None
        self._original_text = text
        
        # ì„œë¸Œì¿¼ë¦¬ ìœ„ì¹˜ë“¤ì„ ì°¾ì•„ì„œ ì €ì¥
        subquery_ranges = []
        
        while not self.is_at_end():
            if self.current_token.type == TokenType.LPAREN:
                # ê´„í˜¸ ì‹œì‘ ìœ„ì¹˜
                start_pos = self.current_token.position
                self.advance()
                
                # ë‹¤ìŒ í† í°ì´ SELECTì¸ì§€ í™•ì¸
                if (not self.is_at_end() and 
                    self.current_token.type == TokenType.KEYWORD and 
                    self.current_token.value.upper() == 'SELECT'):
                    
                    # ì„œë¸Œì¿¼ë¦¬ ì‹œì‘ - ë§¤ì¹­ë˜ëŠ” ê´„í˜¸ê¹Œì§€ ì°¾ê¸°
                    paren_count = 1
                    
                    while not self.is_at_end() and paren_count > 0:
                        if self.current_token.type == TokenType.LPAREN:
                            paren_count += 1
                        elif self.current_token.type == TokenType.RPAREN:
                            paren_count -= 1
                            if paren_count == 0:
                                # ì„œë¸Œì¿¼ë¦¬ ë ìœ„ì¹˜
                                end_pos = self.current_token.position + len(self.current_token.value)
                                subquery_ranges.append((start_pos, end_pos))
                                break
                        self.advance()
            else:
                self.advance()
        
        # ë’¤ì—ì„œë¶€í„° ëŒ€ì²´ (ìœ„ì¹˜ê°€ ë³€ê²½ë˜ì§€ ì•Šë„ë¡)
        result = text
        for start_pos, end_pos in reversed(subquery_ranges):
            result = result[:start_pos] + '1' + result[end_pos:]
        
        return result
    
    def remove_as_aliases(self, text: str) -> str:
        """íŒŒì„œë¥¼ ì‚¬ìš©í•´ì„œ AS ë³„ì¹­ì„ ì œê±°"""
        self.tokens = self.tokenizer.tokenize(text)
        self.position = 0
        self.current_token = self.tokens[0] if self.tokens else None
        self._original_text = text
        
        # AS ì ˆ ìœ„ì¹˜ë“¤ì„ ì°¾ì•„ì„œ ì €ì¥
        as_ranges = []
        
        while not self.is_at_end():
            if (self.current_token.type == TokenType.KEYWORD and 
                self.current_token.value.upper() == 'AS'):
                
                # AS ì‹œì‘ ìœ„ì¹˜
                start_pos = self.current_token.position
                self.advance()
                
                # ë³„ì¹­ í™•ì¸
                if (not self.is_at_end() and 
                    self.current_token.type in [TokenType.IDENTIFIER, TokenType.STRING]):
                    # AS ë³„ì¹­ ë ìœ„ì¹˜
                    end_pos = self.current_token.position + len(self.current_token.value)
                    as_ranges.append((start_pos, end_pos))
                    self.advance()
            else:
                self.advance()
        
        # ë’¤ì—ì„œë¶€í„° ì œê±° (ìœ„ì¹˜ê°€ ë³€ê²½ë˜ì§€ ì•Šë„ë¡)
        result = text
        for start_pos, end_pos in reversed(as_ranges):
            result = result[:start_pos] + result[end_pos:]
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        result = re.sub(r'\s+', ' ', result)
        result = re.sub(r'\s*,\s*', ', ', result)
        
        return result.strip()
    
    def fix_special_functions(self, text: str) -> str:
        """CAST í•¨ìˆ˜ì™€ ìœˆë„ìš° í•¨ìˆ˜ íŠ¹ë³„ ì²˜ë¦¬ (PARTITION BY ì ˆ ì²˜ë¦¬ ê°œì„ )"""
        result = text
        
        # 1. CAST í•¨ìˆ˜ ê°œì„ ëœ ì²˜ë¦¬
        result = self.fix_cast_functions(result)
        
        # 2. PARTITION BY ë³´í˜¸: 1 BY â†’ PARTITION BY
        result = re.sub(r'\b1\s+BY\b', 'PARTITION BY', result, flags=re.IGNORECASE)
        
        # 3. Oracle í•¨ìˆ˜ë¥¼ MySQL í˜¸í™˜ í•¨ìˆ˜ë¡œ ëŒ€ì²´
        # RATIO_TO_REPORT(ê°’) OVER(...) â†’ (ê°’ / SUM(ê°’) OVER(...))
        result = re.sub(r'RATIO_TO_REPORT\s*\(\s*([^)]+)\s*\)\s*(OVER\s*\([^)]*\))', 
                       r'(\1 / SUM(\1) \2)', result, flags=re.IGNORECASE)
        
        # GROUPING(ì»¬ëŸ¼) â†’ 0 (MySQLì—ì„œëŠ” GROUPING í•¨ìˆ˜ê°€ ì—†ìœ¼ë¯€ë¡œ 0ìœ¼ë¡œ ëŒ€ì²´)
        result = re.sub(r'GROUPING\s*\([^)]*\)', '0', result, flags=re.IGNORECASE)
        
        # 4. ìœˆë„ìš° í•¨ìˆ˜ ì²˜ë¦¬ ê°œì„ 
        # ROW_NUMBER() í•¨ìˆ˜ê°€ ì˜ëª»ëœ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ê²½ìš° 1ë¡œ ëŒ€ì²´
        result = re.sub(r'ROW_NUMBER\s*\(\s*\)\s*OVER\s*\([^)]*\)', '1', result, flags=re.IGNORECASE)
        
        # PARTITION BY ì ˆì—ì„œ 1 ì²˜ë¦¬: PARTITION BY (SELECT NULL), 1 â†’ PARTITION BY (SELECT NULL)
        result = re.sub(r'PARTITION\s+BY\s+\(SELECT\s+NULL\)\s*,\s*1\b', 'PARTITION BY (SELECT NULL)', result, flags=re.IGNORECASE)
        
        # ORDER BY ì ˆì—ì„œ 1ì´ ì—¬ëŸ¬ ê°œ ìˆëŠ” ê²½ìš° â†’ ORDER BY (SELECT NULL)
        result = re.sub(r'ORDER\s+BY\s+(?:\(SELECT\s+NULL\)\s*,\s*)*1(?:\s*,\s*1)*\b', 
                       'ORDER BY (SELECT NULL)', result, flags=re.IGNORECASE)
        
        # ORDER BY ë³„ì¹­ ì²˜ë¦¬: ORDER BY YEAR â†’ ORDER BY (SELECT NULL)
        result = re.sub(r'ORDER\s+BY\s+[A-Z_][A-Z0-9_]*\b', 'ORDER BY (SELECT NULL)', result, flags=re.IGNORECASE)
        
        # ê¸°ë³¸ ORDER BY 1 â†’ ORDER BY (SELECT NULL)
        result = re.sub(r'ORDER\s+BY\s+1\b', 'ORDER BY (SELECT NULL)', result, flags=re.IGNORECASE)
        
        # 5. PARTITION BY 1 â†’ PARTITION BY (SELECT NULL)  
        result = re.sub(r'PARTITION\s+BY\s+1\b', 'PARTITION BY (SELECT NULL)', result, flags=re.IGNORECASE)
        
        return result
    
    def fix_cast_functions(self, text: str) -> str:
        """CAST í•¨ìˆ˜ ì „ìš© ê°œì„ ëœ íŒŒì‹± ì²˜ë¦¬"""
        result = text
        
        # CAST í•¨ìˆ˜ íŒ¨í„´ ë§¤ì¹­ ë° ì˜¬ë°”ë¥¸ êµ¬ë¬¸ ìƒì„±
        # íŒ¨í„´: CAST(expression AS datatype) í˜•íƒœ ìœ ì§€
        
        # 1. ì´ë¯¸ AS ì ˆì´ ìˆëŠ” CAST í•¨ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
        cast_with_as_pattern = r'CAST\s*\(\s*([^)]+?)\s+AS\s+[^)]+\s*\)'
        cast_matches_with_as = re.findall(cast_with_as_pattern, result, flags=re.IGNORECASE)
        
        # 2. AS ì ˆì´ ì—†ëŠ” CAST í•¨ìˆ˜ì—ë§Œ AS CHAR(255) ì¶”ê°€
        # ë‹¨, ë‚´ë¶€ í•¨ìˆ˜ì˜ AS ì ˆê³¼ í˜¼ë™í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜
        def fix_cast_without_as(match):
            inner_expr = match.group(1).strip()
            # ë‚´ë¶€ì— ì´ë¯¸ ASê°€ ìˆëŠ”ì§€ í™•ì¸ (SUBSTRING(..., ... AS ...) ê°™ì€ ê²½ìš°)
            if ' AS ' in inner_expr.upper():
                # ë‚´ë¶€ AS ì ˆì„ ì œê±°í•˜ê³  ì™¸ë¶€ì— AS CHAR(255) ì¶”ê°€
                # SUBSTRING(1, 16, 3 AS CHAR(255)) â†’ SUBSTRING(1, 16, 3)
                inner_expr = re.sub(r'\s+AS\s+[^,)]+', '', inner_expr, flags=re.IGNORECASE)
            return f'CAST({inner_expr} AS CHAR(255))'
        
        # AS ì ˆì´ ì—†ëŠ” CAST í•¨ìˆ˜ ì²˜ë¦¬
        cast_without_as_pattern = r'CAST\s*\(\s*([^)]+?)\s*\)(?!\s+AS)'
        result = re.sub(cast_without_as_pattern, fix_cast_without_as, result, flags=re.IGNORECASE)
        
        return result
    
    def fix_other_special_functions(self, text: str) -> str:
        """CAST í•¨ìˆ˜ ì™¸ì˜ íŠ¹ë³„ í•¨ìˆ˜ ì²˜ë¦¬ (ìœˆë„ìš° í•¨ìˆ˜, Oracle í•¨ìˆ˜)"""
        result = text
        
        # 2. PARTITION BY ë³´í˜¸: 1 BY â†’ PARTITION BY
        result = re.sub(r'\b1\s+BY\b', 'PARTITION BY', result, flags=re.IGNORECASE)
        
        # 3. ORDER BY ë³´í˜¸: 1 BY â†’ ORDER BY  
        result = re.sub(r'\bORDER\s+1\b', 'ORDER BY', result, flags=re.IGNORECASE)
        
        # 4. Oracle í•¨ìˆ˜ ë³€í™˜
        result = re.sub(r'\bNVL\s*\(', 'IFNULL(', result, flags=re.IGNORECASE)
        result = re.sub(r'\bTO_CHAR\s*\(', 'CAST(', result, flags=re.IGNORECASE)
        result = re.sub(r'\bTO_NUMBER\s*\(', 'CAST(', result, flags=re.IGNORECASE)
        
        # 5. PARTITION BY 1 â†’ PARTITION BY (SELECT NULL)  
        result = re.sub(r'PARTITION\s+BY\s+1\b', 'PARTITION BY (SELECT NULL)', result, flags=re.IGNORECASE)
        
        return result
    
    def parse_trim_function(self, start_pos):
        """TRIM í•¨ìˆ˜ ì „ìš© íŒŒì‹± - LEADING/TRAILING/BOTH FROM êµ¬ì¡° ë³´ì¡´"""
        paren_count = 0
        
        while not self.is_at_end():
            if self.current_token.type == TokenType.LPAREN:
                paren_count += 1
            elif self.current_token.type == TokenType.RPAREN:
                paren_count -= 1
                if paren_count == 0:
                    # TRIM í•¨ìˆ˜ì˜ ë‹«ëŠ” ê´„í˜¸ ì°¾ìŒ
                    end_pos = self.current_token.position + len(self.current_token.value)
                    self.advance()
                    
                    # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ TRIM í•¨ìˆ˜ ì¶”ì¶œ
                    original_text = self.get_original_text()
                    if start_pos < len(original_text) and end_pos <= len(original_text):
                        trim_function = original_text[start_pos:end_pos].strip()
                        return trim_function
                    break
            
            self.advance()
        
        return None
    
    def fix_cast_and_window_functions(self, text: str) -> str:
        """CAST í•¨ìˆ˜ì™€ ìœˆë„ìš° í•¨ìˆ˜ íŠ¹ë³„ ì²˜ë¦¬"""
        result = text
        
        # 1. CAST í•¨ìˆ˜ ìˆ˜ì •: CAST(ê°’ ) â†’ CAST(ê°’ AS VARCHAR(255))
        result = re.sub(r'CAST\s*\(\s*([^)]+?)\s*\)', r'CAST(\1 AS VARCHAR(255))', result, flags=re.IGNORECASE)
        
        # 2. ìœˆë„ìš° í•¨ìˆ˜ ORDER BY ìˆ˜ì •: ORDER BY 1 â†’ ORDER BY NULL
        result = re.sub(r'ORDER\s+BY\s+1\b', 'ORDER BY NULL', result, flags=re.IGNORECASE)
        
        # 3. PARTITION BY ìˆ˜ì •: PARTITION BY 1 â†’ PARTITION BY NULL  
        result = re.sub(r'PARTITION\s+BY\s+1\b', 'PARTITION BY NULL', result, flags=re.IGNORECASE)
        
        return result

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_parser():
    """íŒŒì„œ í…ŒìŠ¤íŠ¸"""
    parser = SQLFunctionParser()
    
    test_sql = """
    SELECT 
        IFNULL(
            (SELECT X.BPLC_NM FROM TB_COM_CD320 X WHERE X.AGT_CD = A.AGT_CD),
            CASE 
                WHEN A.AGT_CD = 'SELK138AX' 
                THEN (SELECT TB320.BPLC_NM FROM TB_COM_CD320 TB320)
                ELSE IFNULL(A.BPLC_CD,'-')
            END
        ) AS BPLC_NM,
        COUNT(*) AS CNT,
        MAX(DATE_FORMAT(NOW(), '%Y%m%d')) AS MAX_DATE
    """
    
    parser.set_original_text(test_sql)
    functions = parser.parse(test_sql)
    
    print("=== íŒŒì„œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
    for i, func in enumerate(functions, 1):
        print(f"{i}. {func}")
    
    return functions

if __name__ == "__main__":
    test_parser()
