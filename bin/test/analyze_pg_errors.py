#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
PostgreSQL ì—ëŸ¬ ìœ í˜• ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
psql ì‹¤í–‰ ê²°ê³¼ì—ì„œ ì—ëŸ¬ ìœ í˜•ì„ ë¶„ì„í•˜ê³  ë¶„ë¥˜í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
  python3 analyze_pg_errors.py                    # ì „ì²´ ì—ëŸ¬ ë¶„ì„
  python3 analyze_pg_errors.py --missing=t        # ëˆ„ë½ëœ í…Œì´ë¸” ë¶„ì„
  python3 analyze_pg_errors.py --missing=s        # ëˆ„ë½ëœ ìŠ¤í‚¤ë§ˆ ë¶„ì„
  python3 analyze_pg_errors.py --missing=f        # ëˆ„ë½ëœ í•¨ìˆ˜ ë¶„ì„
  python3 analyze_pg_errors.py --missing=o        # ëˆ„ë½ëœ ì˜¤í¼ë ˆì´í„° ë¶„ì„
"""

import re
import json
import sys
import argparse
from collections import defaultdict, Counter
from datetime import datetime
import subprocess

def get_pg_errors():
    """PostgreSQL ì—ëŸ¬ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    query = """
    select pg_file_path, pg_result 
    from sqllist 
    where sql_id != 'retrieveCsrBatchJobMst.csrBatchJobRegDao.retrieveCsrBatchJobMst' 
    and same='N' 
    and pg_result like '%ERROR%' 
    and pg_result not like '%ERROR:  schema%' 
    and pg_result not like '%ERROR:  relation%' 
    and pg_result not like '%ERROR:  operator%' 
    and pg_result not like '%ERROR:  function%';
    """
    
    try:
        result = subprocess.run(['psql', '-c', query], 
                              capture_output=True, text=True, check=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        print(f"PostgreSQL ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return ""

def get_all_pg_errors():
    """ëª¨ë“  PostgreSQL ì—ëŸ¬ ê²°ê³¼ë¥¼ íŒŒì¼ ê²½ë¡œì™€ í•¨ê»˜ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    query = """
    select pg_file_path, pg_result 
    from sqllist 
    where sql_id != 'retrieveCsrBatchJobMst.csrBatchJobRegDao.retrieveCsrBatchJobMst' 
    and same='N' 
    and pg_result like '%ERROR%';
    """
    
    try:
        result = subprocess.run(['psql', '-c', query], 
                              capture_output=True, text=True, check=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        print(f"PostgreSQL ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return ""

def parse_error_data(pg_output):
    """PostgreSQL ì¶œë ¥ì—ì„œ íŒŒì¼ ê²½ë¡œì™€ ì—ëŸ¬ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
    lines = pg_output.strip().split('\n')
    error_data = []
    
    for line in lines:
        if '|' in line and 'ERROR:' in line:
            parts = line.split('|', 1)
            if len(parts) == 2:
                file_path = parts[0].strip()
                error_result = parts[1].strip()
                
                # ERROR: ë¡œ ì‹œì‘í•˜ëŠ” ì—ëŸ¬ ë©”ì‹œì§€ ì¶”ì¶œ
                error_matches = re.findall(r'ERROR:\s*([^\n]+)', error_result)
                for error_msg in error_matches:
                    # íŒŒì¼ ê²½ë¡œì™€ ë¼ì¸ ë²ˆí˜¸ ì œê±°
                    clean_error = re.sub(r'^psql:[^:]+:\d+:\s*', '', error_msg)
                    clean_error = re.sub(r'^ERROR:\s*', '', clean_error)
                    error_data.append({
                        'file_path': file_path,
                        'error': clean_error.strip()
                    })
    
    return error_data

def categorize_errors_with_files(error_data):
    """ì—ëŸ¬ë“¤ì„ ìœ í˜•ë³„ë¡œ ë¶„ë¥˜í•˜ê³  íŒŒì¼ ê²½ë¡œë„ í•¨ê»˜ ì €ì¥í•©ë‹ˆë‹¤."""
    error_categories = defaultdict(lambda: {'errors': [], 'files': set()})
    
    for item in error_data:
        error = item['error']
        file_path = item['file_path']
        
        # ì—ëŸ¬ ìœ í˜• ë¶„ë¥˜ (ë²ˆí˜¸ í¬í•¨)
        if 'relation' in error and 'does not exist' in error:
            category = '01. Relation Not Found'
        elif 'schema' in error and 'does not exist' in error:
            category = '02. Schema Not Found'
        elif 'function' in error and 'does not exist' in error:
            category = '03. Function Not Found'
        elif 'operator does not exist' in error:
            category = '04. Operator Not Found'
        elif 'subquery in FROM must have an alias' in error:
            category = '05. Subquery Alias Missing'
        elif 'syntax error at or near' in error:
            category = '06. Syntax Error'
        elif 'cross-database references are not implemented' in error:
            category = '07. Cross-Database Reference'
        elif 'COALESCE types' in error and 'cannot be matched' in error:
            category = '08. Type Mismatch (COALESCE)'
        elif 'invalid input syntax for type' in error:
            category = '09. Invalid Input Syntax'
        elif 'invalid reference to FROM-clause entry' in error:
            category = '10. Invalid FROM Reference'
        elif 'recursive query' in error and 'column' in error and 'has type' in error:
            category = '11. Recursive Query Type Mismatch'
        elif 'column' in error and 'does not exist' in error:
            category = '12. Column Not Found'
        elif 'missing FROM-clause entry for table' in error:
            category = '13. Missing FROM Clause'
        elif 'procedure' in error and 'does not exist' in error:
            category = '14. Procedure Not Found'
        elif 'must appear in the GROUP BY clause' in error:
            category = '15. GROUP BY Clause Error'
        elif 'date/time field value out of range' in error:
            category = '16. Date Format Error'
        else:
            category = '99. Other Errors'
        
        error_categories[category]['errors'].append(error)
        error_categories[category]['files'].add(file_path)
    
    return error_categories

def get_missing_objects_data(object_type):
    """íŠ¹ì • íƒ€ì…ì˜ ëˆ„ë½ëœ ì˜¤ë¸Œì íŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    type_conditions = {
        't': "pg_result like '%ERROR:  relation%does not exist%'",
        's': "pg_result like '%ERROR:  schema%does not exist%'", 
        'f': "pg_result like '%ERROR:  function%does not exist%'",
        'o': "pg_result like '%ERROR:  operator does not exist%'"
    }
    
    if object_type not in type_conditions:
        return ""
    
    query = f"""
    select pg_file_path, pg_result 
    from sqllist 
    where sql_id != 'retrieveCsrBatchJobMst.csrBatchJobRegDao.retrieveCsrBatchJobMst' 
    and same='N' 
    and {type_conditions[object_type]};
    """
    
    try:
        result = subprocess.run(['psql', '-c', query], 
                              capture_output=True, text=True, check=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        print(f"PostgreSQL ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return ""

def extract_missing_objects(pg_output, object_type):
    """ëˆ„ë½ëœ ì˜¤ë¸Œì íŠ¸ë“¤ì„ ì¶”ì¶œí•˜ê³  ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    lines = pg_output.strip().split('\n')
    missing_objects = defaultdict(lambda: {'files': set(), 'errors': []})
    
    for line in lines:
        if '|' in line and 'ERROR:' in line:
            parts = line.split('|', 1)
            if len(parts) == 2:
                file_path = parts[0].strip()
                error_result = parts[1].strip()
                
                # ì˜¤ë¸Œì íŠ¸ ì´ë¦„ ì¶”ì¶œ
                object_names = extract_object_names(error_result, object_type)
                
                for obj_name in object_names:
                    missing_objects[obj_name]['files'].add(file_path)
                    missing_objects[obj_name]['errors'].append(error_result)
    
    return missing_objects

def extract_object_names(error_text, object_type):
    """ì—ëŸ¬ í…ìŠ¤íŠ¸ì—ì„œ ì˜¤ë¸Œì íŠ¸ ì´ë¦„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    object_names = []
    
    if object_type == 't':  # í…Œì´ë¸”/ë·°
        # relation "table_name" does not exist
        matches = re.findall(r'relation "([^"]+)" does not exist', error_text)
        object_names.extend(matches)
        
    elif object_type == 's':  # ìŠ¤í‚¤ë§ˆ
        # schema "schema_name" does not exist
        matches = re.findall(r'schema "([^"]+)" does not exist', error_text)
        object_names.extend(matches)
        
    elif object_type == 'f':  # í•¨ìˆ˜
        # function function_name(...) does not exist
        matches = re.findall(r'function ([^(]+)\([^)]*\) does not exist', error_text)
        object_names.extend(matches)
        
    elif object_type == 'o':  # ì˜¤í¼ë ˆì´í„°
        # operator does not exist: type1 operator type2
        matches = re.findall(r'operator does not exist: ([^:]+)', error_text)
        object_names.extend(matches)
    
    return object_names

def analyze_missing_objects(object_type):
    """ëˆ„ë½ëœ ì˜¤ë¸Œì íŠ¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    type_names = {
        't': 'í…Œì´ë¸”/ë·°',
        's': 'ìŠ¤í‚¤ë§ˆ', 
        'f': 'í•¨ìˆ˜',
        'o': 'ì˜¤í¼ë ˆì´í„°'
    }
    
    type_name = type_names.get(object_type, 'ì•Œ ìˆ˜ ì—†ìŒ')
    
    print("=" * 80)
    print(f"ëˆ„ë½ëœ {type_name} ìƒì„¸ ë¶„ì„")
    print("=" * 80)
    
    # ë°ì´í„° ìˆ˜ì§‘
    print(f"ëˆ„ë½ëœ {type_name} ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    pg_output = get_missing_objects_data(object_type)
    
    if not pg_output:
        print(f"ëˆ„ë½ëœ {type_name} ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì˜¤ë¸Œì íŠ¸ ì¶”ì¶œ ë° ë¶„ë¥˜
    missing_objects = extract_missing_objects(pg_output, object_type)
    
    if not missing_objects:
        print(f"ëˆ„ë½ëœ {type_name}ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ì´ {len(missing_objects)}ê°œì˜ ëˆ„ë½ëœ {type_name} ë°œê²¬")
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ëˆ„ë½ëœ {type_name} ëª©ë¡:")
    print("=" * 60)
    
    # íŒŒì¼ ê°œìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_objects = sorted(missing_objects.items(), 
                          key=lambda x: len(x[1]['files']), reverse=True)
    
    for i, (obj_name, obj_data) in enumerate(sorted_objects, 1):
        file_count = len(obj_data['files'])
        error_count = len(obj_data['errors'])
        
        print(f"\nğŸ”¸ {i:2d}. {obj_name}")
        print(f"     ì—ëŸ¬ ë°œìƒ: {error_count}íšŒ, ì˜í–¥ íŒŒì¼: {file_count}ê°œ")
        
        # ëŒ€í‘œ ì—ëŸ¬ ë©”ì‹œì§€ (ì²« ë²ˆì§¸)
        if obj_data['errors']:
            first_error = obj_data['errors'][0]
            # ERROR ë¶€ë¶„ë§Œ ì¶”ì¶œ
            error_match = re.search(r'ERROR:\s*([^\n]+)', first_error)
            if error_match:
                clean_error = error_match.group(1)
                # íŒŒì¼ ê²½ë¡œì™€ ë¼ì¸ ë²ˆí˜¸ ì œê±°
                clean_error = re.sub(r'^psql:[^:]+:\d+:\s*ERROR:\s*', '', clean_error)
                print(f"     ì—ëŸ¬: {clean_error}")
    
    # ìƒì„¸ íŒŒì¼ ëª©ë¡
    print(f"\n" + "=" * 80)
    print(f"ğŸ“ ëˆ„ë½ëœ {type_name}ë³„ ìƒì„¸ íŒŒì¼ ëª©ë¡")
    print("=" * 80)
    
    for i, (obj_name, obj_data) in enumerate(sorted_objects, 1):
        file_count = len(obj_data['files'])
        
        print(f"\nğŸ”¸ {i:2d}. {obj_name} ({file_count}ê°œ íŒŒì¼)")
        print("-" * 70)
        
        # íŒŒì¼ ê²½ë¡œë¥¼ ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬í•´ì„œ ì¶œë ¥
        sorted_files = sorted(list(obj_data['files']))
        for j, file_path in enumerate(sorted_files, 1):
            print(f"     {j:2d}. {file_path}")
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    analysis_result = {
        'analysis_date': datetime.now().isoformat(),
        'object_type': type_name,
        'total_missing_objects': len(missing_objects),
        'missing_objects': {
            obj_name: {
                'error_count': len(obj_data['errors']),
                'file_count': len(obj_data['files']),
                'files': sorted(list(obj_data['files'])),
                'sample_errors': obj_data['errors'][:3]  # ìƒ˜í”Œ ì—ëŸ¬ 3ê°œ
            }
            for obj_name, obj_data in missing_objects.items()
        }
    }
    
    output_file = f"/tmp/missing_{object_type}_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ìƒì„¸ ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")
    
    # ìš”ì•½
    print(f"\nğŸ“‹ ìš”ì•½:")
    print(f"   - ëˆ„ë½ëœ {type_name} ìˆ˜: {len(missing_objects)}ê°œ")
    total_files = len(set().union(*[obj_data['files'] for obj_data in missing_objects.values()]))
    print(f"   - ì˜í–¥ë°›ëŠ” íŒŒì¼ ìˆ˜: {total_files}ê°œ")
    if missing_objects:
        most_used = max(missing_objects.items(), key=lambda x: len(x[1]['files']))
        print(f"   - ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ {type_name}: {most_used[0]} ({len(most_used[1]['files'])}ê°œ íŒŒì¼)")

def get_error_type_mapping():
    """ì—ëŸ¬ ìœ í˜• ë²ˆí˜¸ì™€ ì´ë¦„ì˜ ë§¤í•‘ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return {
        '01': 'Relation Not Found',
        '02': 'Schema Not Found', 
        '03': 'Function Not Found',
        '04': 'Operator Not Found',
        '05': 'Subquery Alias Missing',
        '06': 'Syntax Error',
        '07': 'Cross-Database Reference',
        '08': 'Type Mismatch (COALESCE)',
        '09': 'Invalid Input Syntax',
        '10': 'Invalid FROM Reference',
        '11': 'Recursive Query Type Mismatch',
        '12': 'Column Not Found',
        '13': 'Missing FROM Clause',
        '14': 'Procedure Not Found',
        '15': 'GROUP BY Clause Error',
        '16': 'Date Format Error',
        '99': 'Other Errors'
    }

def analyze_specific_patterns(pg_output):
    """íŠ¹ì • ì—ëŸ¬ íŒ¨í„´ì„ ë” ìì„¸íˆ ë¶„ì„í•©ë‹ˆë‹¤."""
    patterns = {
        'TABLE function usage': r'FROM TABLE\(',
        'Database link (@)': r'@[A-Z_]+',
        'Oracle-style LIMIT': r'and limit \d+',
        'Stored procedure call': r'call [A-Z_]+\.',
        'JSON/CLOB syntax': r'\{[^}]*\}',
        'Missing column in bind': r'AND [A-Z]\. = ',
    }
    
    pattern_counts = {}
    for pattern_name, pattern in patterns.items():
        matches = re.findall(pattern, pg_output, re.IGNORECASE)
        pattern_counts[pattern_name] = len(matches)
    
    return pattern_counts

def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
    parser = argparse.ArgumentParser(
        description='PostgreSQL ì—ëŸ¬ ìœ í˜• ë¶„ì„ ë„êµ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python3 analyze_pg_errors.py                    # ì „ì²´ ì—ëŸ¬ ë¶„ì„
  python3 analyze_pg_errors.py --missing=t        # ëˆ„ë½ëœ í…Œì´ë¸” ë¶„ì„
  python3 analyze_pg_errors.py --missing=s        # ëˆ„ë½ëœ ìŠ¤í‚¤ë§ˆ ë¶„ì„
  python3 analyze_pg_errors.py --missing=f        # ëˆ„ë½ëœ í•¨ìˆ˜ ë¶„ì„
  python3 analyze_pg_errors.py --missing=o        # ëˆ„ë½ëœ ì˜¤í¼ë ˆì´í„° ë¶„ì„
  python3 analyze_pg_errors.py --types             # ì—ëŸ¬ ìœ í˜• ë²ˆí˜¸ ë§¤í•‘ í‘œì‹œ
        """
    )
    
    parser.add_argument(
        '--missing',
        choices=['t', 's', 'f', 'o'],
        help='ëˆ„ë½ëœ ì˜¤ë¸Œì íŠ¸ ë¶„ì„ (t:í…Œì´ë¸”, s:ìŠ¤í‚¤ë§ˆ, f:í•¨ìˆ˜, o:ì˜¤í¼ë ˆì´í„°)'
    )
    
    parser.add_argument(
        '--types',
        action='store_true',
        help='ì—ëŸ¬ ìœ í˜• ë²ˆí˜¸ ë§¤í•‘ í‘œì‹œ'
    )
    
    return parser.parse_args()

def show_error_types():
    """ì—ëŸ¬ ìœ í˜• ë²ˆí˜¸ ë§¤í•‘ì„ í‘œì‹œí•©ë‹ˆë‹¤."""
    print("=" * 80)
    print("PostgreSQL ì—ëŸ¬ ìœ í˜• ë²ˆí˜¸ ë§¤í•‘")
    print("=" * 80)
    
    error_mapping = get_error_type_mapping()
    
    print("\nğŸ“‹ ì—ëŸ¬ ìœ í˜• ë²ˆí˜¸:")
    print("-" * 50)
    
    for number, name in sorted(error_mapping.items()):
        print(f"  {number}: {name}")
    
    print(f"\nğŸ’¡ ì‚¬ìš©ë²•:")
    print(f"   - ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì—ì„œ ì´ ë²ˆí˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì—ëŸ¬ ìœ í˜•ì„ ì°¸ì¡°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    print(f"   - JSON ê²°ê³¼ íŒŒì¼ì—ì„œ 'error_number' í•„ë“œë¡œ ë²ˆí˜¸ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±
    args = parse_arguments()
    
    # ì—ëŸ¬ ìœ í˜• ë²ˆí˜¸ ë§¤í•‘ í‘œì‹œ ëª¨ë“œ
    if args.types:
        show_error_types()
        return
    
    # ëˆ„ë½ëœ ì˜¤ë¸Œì íŠ¸ ë¶„ì„ ëª¨ë“œ
    if args.missing:
        analyze_missing_objects(args.missing)
        return
    
    # ê¸°ë³¸ ì „ì²´ ì—ëŸ¬ ë¶„ì„ ëª¨ë“œ
    print("=" * 80)
    print("PostgreSQL ì—ëŸ¬ ìœ í˜• ë¶„ì„")
    print("=" * 80)
    
    # ëª¨ë“  PostgreSQL ì—ëŸ¬ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    print("PostgreSQL ì—ëŸ¬ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    all_pg_output = get_all_pg_errors()
    
    if not all_pg_output:
        print("ì—ëŸ¬ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì—ëŸ¬ ë°ì´í„° íŒŒì‹±
    error_data = parse_error_data(all_pg_output)
    print(f"ì´ {len(error_data)}ê°œì˜ ì—ëŸ¬ ë°œê²¬")
    
    # ì—ëŸ¬ ë¶„ë¥˜ (íŒŒì¼ ê²½ë¡œ í¬í•¨)
    error_categories = categorize_errors_with_files(error_data)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ì—ëŸ¬ ìœ í˜•ë³„ ë¶„ì„ ê²°ê³¼:")
    print("=" * 50)
    
    total_errors = sum(len(cat_data['errors']) for cat_data in error_categories.values())
    
    for category, cat_data in sorted(error_categories.items(), 
                                   key=lambda x: len(x[1]['errors']), reverse=True):
        count = len(cat_data['errors'])
        file_count = len(cat_data['files'])
        percentage = (count / total_errors) * 100 if total_errors > 0 else 0
        
        print(f"\nğŸ”¸ {category}: {count}ê°œ ({percentage:.1f}%) - {file_count}ê°œ íŒŒì¼")
        
        # ê° ì¹´í…Œê³ ë¦¬ì˜ ëŒ€í‘œ ì—ëŸ¬ ì˜ˆì‹œ (ìµœëŒ€ 3ê°œ)
        for i, error in enumerate(cat_data['errors'][:3]):
            print(f"   {i+1}. {error[:100]}{'...' if len(error) > 100 else ''}")
        
        if len(cat_data['errors']) > 3:
            print(f"   ... ì™¸ {len(cat_data['errors']) - 3}ê°œ ë”")
    
    # íŠ¹ì • íŒ¨í„´ ë¶„ì„
    print(f"\nğŸ” íŠ¹ì • íŒ¨í„´ ë¶„ì„:")
    print("=" * 30)
    
    pattern_counts = analyze_specific_patterns(all_pg_output)
    for pattern_name, count in sorted(pattern_counts.items(), 
                                    key=lambda x: x[1], reverse=True):
        if count > 0:
            print(f"   {pattern_name}: {count}ê°œ")
    
    # ìƒì„¸ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    analysis_result = {
        'analysis_date': datetime.now().isoformat(),
        'total_errors': total_errors,
        'error_type_mapping': get_error_type_mapping(),
        'error_categories': {
            category: {
                'error_number': category.split('.')[0],
                'error_name': category.split('. ', 1)[1] if '. ' in category else category,
                'count': len(cat_data['errors']),
                'file_count': len(cat_data['files']),
                'percentage': (len(cat_data['errors']) / total_errors) * 100 if total_errors > 0 else 0,
                'examples': cat_data['errors'][:5],  # ìƒìœ„ 5ê°œ ì˜ˆì‹œ
                'files': sorted(list(cat_data['files']))  # íŒŒì¼ ëª©ë¡
            }
            for category, cat_data in error_categories.items()
        },
        'pattern_analysis': pattern_counts
    }
    
    output_file = f"/tmp/pg_error_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ìƒì„¸ ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")
    
    # ìš”ì•½
    print(f"\nğŸ“‹ ìš”ì•½:")
    print(f"   - ì´ ì—ëŸ¬ ìˆ˜: {total_errors}ê°œ")
    print(f"   - ì£¼ìš” ì—ëŸ¬ ìœ í˜•: {len(error_categories)}ê°œ ì¹´í…Œê³ ë¦¬")
    print(f"   - ê°€ì¥ ë§ì€ ì—ëŸ¬: {max(error_categories.items(), key=lambda x: len(x[1]['errors']))[0]} ({max(len(cat_data['errors']) for cat_data in error_categories.values())}ê°œ)")
    
    # ë³„ì²¨: ìœ í˜•ë³„ ì—ëŸ¬ íŒŒì¼ ê²½ë¡œ
    print(f"\n" + "=" * 80)
    print("ğŸ“ ë³„ì²¨: ìœ í˜•ë³„ ì—ëŸ¬ íŒŒì¼ ê²½ë¡œ")
    print("=" * 80)
    
    # CSV íŒŒì¼ ì €ì¥ì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘
    csv_data = []
    
    # íŒŒì¼ë³„ ì—ëŸ¬ ë©”ì‹œì§€ ë§¤í•‘ ìƒì„± (ë” ì •í™•í•œ ë§¤í•‘ì„ ìœ„í•´)
    file_to_errors = {}
    for item in error_data:
        file_path = item['file_path']
        error_msg = item['error']
        if file_path not in file_to_errors:
            file_to_errors[file_path] = []
        file_to_errors[file_path].append(error_msg)
    
    for category, cat_data in sorted(error_categories.items(), 
                                   key=lambda x: len(x[1]['errors']), reverse=True):
        count = len(cat_data['errors'])
        file_count = len(cat_data['files'])
        
        print(f"\nğŸ”¸ {category} ({count}ê°œ ì—ëŸ¬, {file_count}ê°œ íŒŒì¼)")
        print("-" * 60)
        
        # íŒŒì¼ ê²½ë¡œë¥¼ ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬í•´ì„œ ì¶œë ¥
        sorted_files = sorted(list(cat_data['files']))
        
        for i, file_path in enumerate(sorted_files, 1):
            print(f"   {i:2d}. {file_path}")
            
            # í•´ë‹¹ íŒŒì¼ì˜ ì²« ë²ˆì§¸ ì—ëŸ¬ ë©”ì‹œì§€ ì‚¬ìš©
            representative_error = "ì—ëŸ¬ ë©”ì‹œì§€ ì—†ìŒ"
            if file_path in file_to_errors and file_to_errors[file_path]:
                representative_error = file_to_errors[file_path][0]
            
            # CSV ë°ì´í„°ì— ì¶”ê°€ (íŒŒì¼ê²½ë¡œ, ì—ëŸ¬ë©”ì‹œì§€)
            csv_data.append([file_path, representative_error])
        
        if len(sorted_files) == 0:
            print("   (íŒŒì¼ ì—†ìŒ)")
    
    # CSV íŒŒì¼ë¡œ ì €ì¥
    csv_file_path = "/tmp/pg_error.csv"
    try:
        import csv
        with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            # í—¤ë” ì‘ì„±
            writer.writerow(['sqlíŒŒì¼', 'ì—ëŸ¬ë©”ì‹œì§€'])
            # ë°ì´í„° ì‘ì„±
            writer.writerows(csv_data)
        
        print(f"\nğŸ’¾ ì—ëŸ¬ íŒŒì¼ ëª©ë¡ì´ CSVë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {csv_file_path}")
        print(f"   - ì´ {len(csv_data)}ê°œ íŒŒì¼ ì •ë³´ ì €ì¥")
        print(f"   - í˜•ì‹: sqlíŒŒì¼, ì—ëŸ¬ë©”ì‹œì§€")
        
    except Exception as e:
        print(f"\nâŒ CSV íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main()
